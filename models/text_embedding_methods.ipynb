{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meyildirim/Desktop/Notebooks/Learning/master-thesis/etl/.venv/lib/python3.8/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "# For GloVe, additional setup is required.\n",
    "\n",
    "def bow_embeddings(texts):\n",
    "    \"\"\"Generates Bag of Words embeddings.\"\"\"\n",
    "    vectorizer = CountVectorizer()\n",
    "    return vectorizer.fit_transform(texts)\n",
    "\n",
    "def word2vec_embeddings(texts):\n",
    "    \"\"\"Generates Word2Vec embeddings.\"\"\"\n",
    "    tokenized_texts = [text.split() for text in texts]\n",
    "    model = Word2Vec(tokenized_texts, min_count=1)\n",
    "    return model.wv\n",
    "\n",
    "def glove_embeddings(texts):\n",
    "    \"\"\"Generates GloVe embeddings. Implementation required.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Example usage\n",
    "texts = [\"türkiye acil durum yönetim\", \"veri madencilik yapay zeka\"]  # Preprocessed Turkish text data\n",
    "bow_emb = bow_embeddings(texts)\n",
    "w2v_emb = word2vec_embeddings(texts)\n",
    "# glove_emb = glove_embeddings(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "\n",
    "\n",
    "def stem_text(tokens):\n",
    "    stemmer = TurkishStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "# You may need to download specific resources from NLTK\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_numbers(text):\n",
    "    #text = re.sub(r'\\d+\\.\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "def remove_stopwords(tokens):\n",
    "    turkish_stopwords = set(stopwords.words('turkish'))\n",
    "    return [word for word in tokens if word not in turkish_stopwords]\n",
    "def preprocess_text(text):\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = to_lowercase(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = stem_text(tokens)\n",
    "    # Apply additional steps like stemming or lemmatization if needed\n",
    "    return ' '.join(tokens)\n",
    "def preprocess_dataset(dataset):\n",
    "    return [preprocess_text(text) for text in dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "dataset = df_labels.Şikayeti_agg.values.tolist()\n",
    "\n",
    "# Preprocess the dataset\n",
    "preprocessed_dataset = preprocess_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "bow_emb = bow_embeddings(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_emb = word2vec_embeddings(preprocessed_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<359297x36621 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1350479 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_emb.to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Getting all the words in the model\n",
    "words = list(w2v_emb.key_to_index.keys())\n",
    "\n",
    "# Creating a matrix where each row is a vector representation of a word\n",
    "word_vectors_matrix = np.array([w2v_emb[word] for word in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "\n",
    "# Training the Word2Vec model\n",
    "model = gensim.models.Word2Vec(sentences=preprocessed_dataset, vector_size=768, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Now, you can get the vector for each word in your vocabulary\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a vector representation for a whole text element, you can average the vectors of its words.\n",
    "def text_to_vector(text, model):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    if vectors:\n",
    "        # Averaging the vectors\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        # Return a zero vector if there are no words in the model's vocabulary\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "\n",
    "# Example: Convert all text elements to vectors\n",
    "text_vectors = [text_to_vector(text, model) for text in preprocessed_dataset]\n",
    "text_vectors = np.array(text_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(text_vectors, 'w2vec_768dim_emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
